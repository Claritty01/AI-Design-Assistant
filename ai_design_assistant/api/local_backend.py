# ai_design_assistant/api/local_backend.py
from __future__ import annotations

import os
import threading
from pathlib import Path
from typing import Iterator, List

import base64
from io import BytesIO
from PIL import Image

import psutil
import gc
import torch
from transformers import (
    LlavaNextForConditionalGeneration,
    AutoProcessor,
    TextIteratorStreamer,
)

import logging

from ai_design_assistant.core.settings import Settings

_LOGGER = logging.getLogger(__name__)

from ai_design_assistant.core.models import ModelBackend, normalize

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_MODEL_NAME = os.getenv("LOCAL_MODEL_NAME", "neulab/Pangea-7B-hf")
_DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"
_DTYPE      = torch.float16 if _DEVICE == "cuda" else torch.float32
_MAX_TOKENS = int(os.getenv("LOCAL_MAX_NEW_TOKENS", "512"))
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _prepare_processor(proc, patch_size: int = 14) -> None:
    """–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω patch_size (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ HF –æ–ø—É—Å–∫–∞—é—Ç –ø–æ–ª–µ)."""
    if getattr(proc, "patch_size", None) is None:
        proc.patch_size = patch_size
    if getattr(proc.image_processor, "patch_size", None) is None:
        proc.image_processor.patch_size = patch_size

def _decode_data_url(data_url: str) -> Image.Image:
    """data:image/...;base64,.... ‚Üí PIL.Image"""
    if "," in data_url:
        data_url = data_url.split(",", 1)[1]
    img_bytes = base64.b64decode(data_url)
    return Image.open(BytesIO(img_bytes)).convert("RGB")

def _collapse_messages(messages: List[dict]):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ([messages], image | None) ‚Äî –∏ –±–µ—Ä—ë—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É —Ç–æ–ª—å–∫–æ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ user-—Å–æ–æ–±—â–µ–Ω–∏—è."""
    image: Image.Image | None = None
    out = []

    for m in messages:
        content = m["content"]

        if isinstance(content, list):
            parts = []
            for chunk in content:
                if chunk["type"] == "text":
                    parts.append(chunk["text"])
                elif chunk["type"] == "image_url":
                    parts.append("<image>")   # –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º <image>
            content = " ".join(parts)

        out.append({"role": m["role"], "content": content})

    # –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    for m in reversed(messages):
        content = m["content"]
        if isinstance(content, list):
            for chunk in content:
                if chunk["type"] == "image_url":
                    image = _decode_data_url(chunk["image_url"]["url"])
                    break
        if image:
            break

    return out, image


def _build_inputs(self, messages):
    msgs, image = _collapse_messages(messages)
    prompt = self.tokenizer.apply_chat_template(
        msgs, tokenize=False, add_generation_prompt=True
    )
    if image is None:          # —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å (text-only)
        batch = self.tokenizer(prompt, return_tensors="pt")
    else:                      # —Ç–µ–∫—Å—Ç + –∫–∞—Ä—Ç–∏–Ω–∫–∞
        batch = self.processor(text=prompt, images=image, return_tensors="pt")
    return {k: v.to(_DEVICE) for k, v in batch.items()}


class _LocalBackend(ModelBackend):
    name = "local"

    def __init__(self) -> None:
        super().__init__()
        self.unload_mode = Settings.load().local_unload_mode
        self.model = LlavaNextForConditionalGeneration.from_pretrained(
            _MODEL_NAME, torch_dtype=_DTYPE
        ).to(_DEVICE)

        self.processor = AutoProcessor.from_pretrained(_MODEL_NAME)
        self.tokenizer = self.processor.tokenizer
        self.model.resize_token_embeddings(len(self.tokenizer))

        _prepare_processor(self.processor)

    def unload_model(self) -> None:
        unload_mode = Settings.load().local_unload_mode

        if unload_mode == "none":
            _LOGGER.info(f"üö´ –í—ã–≥—Ä—É–∑–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ ‚Äî –º–æ–¥–µ–ª—å –æ—Å—Ç–∞—ë—Ç—Å—è –≤ VRAM")
            return

        if unload_mode == "cpu":
            if torch.cuda.is_available():
                used_before = torch.cuda.memory_allocated() / (1024 ** 2)
                self.model.to("cpu")
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                used_after = torch.cuda.memory_allocated() / (1024 ** 2)
                _LOGGER.info(f"üîã –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –≤ RAM. VRAM –¥–æ: {used_before:.2f} MB ‚Üí –ø–æ—Å–ª–µ: {used_after:.2f} MB")
        elif unload_mode == "full":
            if torch.cuda.is_available():
                used_before = torch.cuda.memory_allocated() / (1024 ** 2)
                torch.cuda.empty_cache()
                _LOGGER.info(f"üóëÔ∏è –ú–æ–¥–µ–ª—å —É–¥–∞–ª–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é. VRAM –±—ã–ª–æ: {used_before:.2f} MB")
            del self.model
            del self.processor
            del self.tokenizer
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            self.model = None

    def _maybe_reload_model(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        if self.model is None:
            _LOGGER.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞...")
            self.model = LlavaNextForConditionalGeneration.from_pretrained(
                _MODEL_NAME, torch_dtype=_DTYPE
            ).to(_DEVICE)
            _LOGGER.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

            self.processor = AutoProcessor.from_pretrained(_MODEL_NAME)
            self.tokenizer = self.processor.tokenizer
            self.model.resize_token_embeddings(len(self.tokenizer))

            _prepare_processor(self.processor)

        elif next(self.model.parameters()).device != torch.device(_DEVICE):
            _LOGGER.info(f"üîÑ –ü–µ—Ä–µ–º–µ—â–∞—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ {_DEVICE}")
            self.model.to(_DEVICE)

    # --- –±–∞–∑–æ–≤—ã–π sync-—Ä–µ–∂–∏–º (–≤–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–∫—É —Ü–µ–ª—å–Ω—ã–º –∫—É—Å–∫–æ–º) -----------------
    def generate(self, messages: List[dict[str, str]], **kw) -> str:
        self._maybe_reload_model()
        batch = _build_inputs(self, messages)
        output = self.model.generate(**batch, max_new_tokens=_MAX_TOKENS)
        gen_ids = output[0][batch["input_ids"].shape[1]:]
        self.unload_model()  # <-- –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–≥—Ä—É–∂–∞–µ–º
        return self.tokenizer.decode(gen_ids, skip_special_tokens=True)

    # --- –ø–æ—Ç–æ–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è (–≤–µ—Ä–Ω—ë—Ç –∏—Ç–µ—Ä–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤) --------------------------
    def stream(self, messages: List[dict[str, str]], **kw) -> Iterator[str]:
        self._maybe_reload_model()
        batch = _build_inputs(self, messages)

        streamer = TextIteratorStreamer(
            self.tokenizer, skip_prompt=True, skip_special_tokens=True
        )
        gen_kwargs = dict(**batch, streamer=streamer, max_new_tokens=_MAX_TOKENS)

        threading.Thread(
            target=self.model.generate, kwargs=gen_kwargs, daemon=True
        ).start()

        for token in streamer:
            yield token

        self.unload_model()  # <-- –ø–æ—Å–ª–µ —Å—Ç—Ä–∏–º–∞ –≤—ã–≥—Ä—É–∂–∞–µ–º


# –≠–∫—Å–ø–æ—Ä—Ç–∏–º –æ–±—ä–µ–∫—Ç, —á—Ç–æ–±—ã api.__init__ —Å–º–æ–≥ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –±–µ–∫–µ–Ω–¥
backend = _LocalBackend()
