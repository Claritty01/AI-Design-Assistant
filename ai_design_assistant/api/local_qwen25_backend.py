# ai_design_assistant/api/local_qwen25_backend.py
from __future__ import annotations
import os, threading, base64, logging
from pathlib import Path
from io import BytesIO
from typing import Iterator, List

from PIL import Image
from qwen_vl_utils import process_vision_info
import torch
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
    TextIteratorStreamer,
    BitsAndBytesConfig
)

from ai_design_assistant.core.settings import Settings
from ai_design_assistant.core.models import ModelBackend, normalize

logging.basicConfig(level=logging.DEBUG)

_LOGGER = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_MODEL_NAME = os.getenv(
    "LOCAL_MODEL_NAME",               # –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
    "Qwen/Qwen2.5-VL-3B-Instruct",
)
_DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"
_DTYPE      = torch.float16 if _DEVICE == "cuda" else torch.float32
_MAX_TOKENS = int(os.getenv("LOCAL_MAX_NEW_TOKENS", "1024"))
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _prepare_processor(proc, patch_size: int = 14):
    """–£ Qwen-VL —É–∂–µ –µ—Å—Ç—å patch_size, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –ø—Ä–æ–≤–µ—Ä–∫—É."""
    if getattr(proc, "patch_size", None) is None:
        proc.patch_size = patch_size
    if getattr(proc.image_processor, "patch_size", None) is None:
        proc.image_processor.patch_size = patch_size


def _decode_data_url(data_url: str) -> str:
    # –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É
    return data_url



def _collapse_messages(raw_messages: List[dict]):
    hf_msgs = []
    for m in raw_messages:
        role = m["role"]
        blocks = []
        content = m["content"]

        if isinstance(content, list):
            for chunk in content:
                if chunk["type"] == "image_url":
                    # ‚¨áÔ∏è  –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –±–ª–æ–∫ type="image" —Å–æ —Å—Ç—Ä–æ–∫–æ–π-URL
                    blocks.append({
                        "type": "image",
                        "image": chunk["image_url"]["url"]    # ‚Üê —Å—Ç—Ä–æ–∫–∞ URL
                    })
                else:         # {"type": "text", ‚Ä¶}
                    blocks.append(chunk)
        else:
            blocks.append({"type": "text", "text": str(content)})
            if m.get("image"):          # –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
                blocks.append({
                    "type": "image",
                    "image": str(Path(m["image"]))            # —Å—Ç—Ä–æ–∫–æ–≤—ã–π –ø—É—Ç—å
                })

        hf_msgs.append({"role": role, "content": blocks})

    # —Ç–µ–ø–µ—Ä—å –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ‚Äî type="image"
    image_inputs, video_inputs = process_vision_info(hf_msgs)
    return hf_msgs, image_inputs, video_inputs




def _build_inputs(self, messages):
    hf_msgs, image_inputs, video_inputs = _collapse_messages(messages)

    # 1) —Ç–µ–∫—Å—Ç —Å <img> —Ç–æ–∫–µ–Ω–∞–º–∏
    prompt = self.processor.apply_chat_template(
        hf_msgs, tokenize=False, add_generation_prompt=True
    )
    _LOGGER.debug(f"Prompt ‚Üí\n{prompt}")

    # 2) –≤ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ processor
    # --- –ö–û–ù–°–¢–†–£–ò–†–£–ï–ú kwargs —Ç–æ–ª—å–∫–æ —Å —Ç–µ–º, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å ---
    proc_kwargs = {
        "text": [prompt],
        "padding": True,
        "return_tensors": "pt",
    }
    if image_inputs:  # –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞
        proc_kwargs["images"] = image_inputs
    if video_inputs:  # –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –≤–∏–¥–µ–æ
        proc_kwargs["videos"] = video_inputs

    inputs = self.processor(**proc_kwargs)

    # 3) –∫ —Ç–æ–º—É –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É
    device = next(self.model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    return inputs


def propagate_last_image(messages: list[dict]) -> list[dict]:
    """
    –ü—Ä–æ–±–µ–≥–∞–µ–º –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –∫ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é –±–µ–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    """
    last_image = None
    new_messages = []
    for msg in messages:
        # –µ—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞ ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –µ—ë
        if msg.get("image"):
            last_image = msg["image"]

        # –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –±–µ–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏ ‚Äî –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º
        if msg["role"] == "user" and not msg.get("image") and last_image:
            msg = msg.copy()
            msg["image"] = last_image

        new_messages.append(msg)

    return new_messages


class _LocalQwenBackend(ModelBackend):
    name = "local_qwen25"

    def __init__(self) -> None:
        super().__init__()
        self.unload_mode = Settings.load().local_unload_mode  # 'none' | 'cpu' | 'full'
        self.model: Qwen2_5_VLForConditionalGeneration | None = None
        self.processor: AutoProcessor | None = None
        self.tokenizer = None

    # ---------------- helpers -----------------
    def _maybe_reload_model(self):
        if self.model is None:
            _LOGGER.info("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é Qwen2.5-VL-3B —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π‚Ä¶")

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                _MODEL_NAME,
                quantization_config=bnb_config,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(
                _MODEL_NAME,
                min_pixels=28 * 28,  # ‚âà 1 –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
                max_pixels=1280 * 28 * 28,  # –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ
            )

            self.tokenizer = self.processor.tokenizer
            self.model.resize_token_embeddings(len(self.tokenizer))
            _prepare_processor(self.processor)
            _LOGGER.info("‚úÖ Qwen2.5-VL-3B –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (4-bit).")
        elif next(self.model.parameters()).device != torch.device(_DEVICE):
            _LOGGER.info(f"üîÑ –ü–µ—Ä–µ–º–µ—â–∞—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ {_DEVICE}")
            self.model.to(_DEVICE)

    def unload_model(self):
        if self.model is None:
            return
        mode = Settings.load().local_unload_mode
        if mode == "none":
            return
        if torch.cuda.is_available():
            before = torch.cuda.memory_allocated() / 1024 ** 2
        if mode == "cpu":
            self.model.to("cpu")
        elif mode == "full":
            del self.model, self.processor, self.tokenizer
            self.model = self.processor = self.tokenizer = None
        import gc, torch as t
        gc.collect()
        t.cuda.empty_cache()
        if torch.cuda.is_available():
            after = torch.cuda.memory_allocated() / 1024 ** 2
            _LOGGER.info(f"üîã VRAM: {before:.1f} MB ‚Üí {after:.1f} MB")

    # -------------- sync --------------
    def generate(self, messages: List[dict], **kw) -> str:
        self._maybe_reload_model()
        messages = propagate_last_image(normalize(messages))
        # –î–æ–±–∞–≤–ª—è–µ–º system-–ø–æ–¥—Å–∫–∞–∑–∫—É:
        messages.insert(0, {
            "role": "system",
            "content": (
                "–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º—É –¥–∏–∑–∞–π–Ω—É. "
                "–ï—Å–ª–∏ –ø–æ–ª—É—á–∞–µ—à—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ª—é–¥—å–º–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É–π –ª–∏—Ü–∞ –∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ UI/UX "
                "–∏–ª–∏ –Ω–∞ —ç—Å—Ç–µ—Ç–∏–∫–µ –∫–∞–¥—Ä–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
            )
        })
        batch = _build_inputs(self, messages)
        output = self.model.generate(
            **batch,
            max_new_tokens=_MAX_TOKENS,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )
        gen_ids = output[0][batch["input_ids"].shape[1]:]
        self.unload_model()
        return self.tokenizer.decode(gen_ids, skip_special_tokens=True)

    # -------------- stream --------------
    def stream(self, messages: List[dict], **kw) -> Iterator[str]:
        self._maybe_reload_model()
        messages = propagate_last_image(normalize(messages))
        # –î–æ–±–∞–≤–ª—è–µ–º system-–ø–æ–¥—Å–∫–∞–∑–∫—É:
        messages.insert(0, {
            "role": "system",
            "content": (
                "–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º—É –¥–∏–∑–∞–π–Ω—É. "
                "–ï—Å–ª–∏ –ø–æ–ª—É—á–∞–µ—à—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ª—é–¥—å–º–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É–π –ª–∏—Ü–∞ –∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ UI/UX "
                "–∏–ª–∏ –Ω–∞ —ç—Å—Ç–µ—Ç–∏–∫–µ –∫–∞–¥—Ä–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
            )
        })
        batch = _build_inputs(self, messages)
        streamer = TextIteratorStreamer(
            self.tokenizer, skip_prompt=True, skip_special_tokens=True
        )
        threading.Thread(
            target=self.model.generate,
            kwargs=dict(
                **batch,
                streamer=streamer,
                max_new_tokens=_MAX_TOKENS,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            ),
            daemon=True,
        ).start()
        for tok in streamer:
            yield tok
        self.unload_model()


# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞
backend = _LocalQwenBackend()

def summarize_chat(prompt: str) -> str:
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π Qwen-VL."""
    return backend.generate([{"role": "user", "content": prompt}])
